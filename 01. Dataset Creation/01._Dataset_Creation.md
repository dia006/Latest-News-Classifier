Creation of the dataset
================

Raw dataset
-----------

The dataset used in this project is the **BBC News Raw Dataset**. It can be downloaded from:

<http://mlg.ucd.ie/datasets/bbc.html>

It consists of 2.225 documents from the BBC news website corresponding to stories in five topical areas from 2004 to 2005. These areas are:

-   Business
-   Entertainment
-   Politics
-   Sport
-   Tech

In the same webpage we can find another dataset (`BBCSport`), which consists of 737 documents from the BBC Sport website. However, in this project it hasn't been used.

In addition, a pre-processed dataset is also provided. This pre-processing includes stemming, stop-word removal and low term frequency filtering. Again, it has not been used. The **raw dataset** has been used instead.

The download file `bbc-fulltext.zip` contains, for each category, every article in a separate `.txt` file.

The aim of this document is to provide explanation on how this information has been assembled into a single dataframe containing the text, category and filename of each article.

Dataset pre-processing
----------------------

Added a Python script to create the dataframe without the need of R language. The sample is pretty neat.

### Output

The output dataframe `News_dataset.csv`contains:

-   `File_Name`: Original name of the article
-   `Content`: Content of the article
-   `Category`: Category of the article
-   `Complete_Filename`: Column created as `File_Name` + `Category`. The objective is to have an unique identifier for each article, since the article file names are repeated across categories.

It has 2.225 rows.
